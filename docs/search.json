[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "Highlights from recent projects & list of publications (see below)\n\n\n\nDeep Facial Phenotyping\n\n\n\n\n\n\n Ö. Sümer, R. L. Waikel, S. E. L. Hanchard, D. Duong, P. Krawitz, C. Conati, B. D. Solomon, E. André, “Region-based saliency explanations on the recognition of facial genetic syndromes,” in Machine learning for healthcare conference, PMLR, 2023. \nPaper Poster Database Code\n\nAbstract: Deep neural networks in computer vision have shown remarkable progress in recogniz- ing facial genetic syndromes. Many genetic syndromes are difficult to detect, even for experienced clinicians, and computer-aided phenotyping can accelerate clinical diagnosis. High-stakes clinical tasks using deep learning, as in clinical genetics, require human un- derstandable explanations for model decisions. Saliency methods are used to explain DNN predictions in various image analysis domains but have yet to be studied in facial genetics. The syndromic features of most genetic conditions are often localized to areas like the eyes, nose, and mouth. In this paper, to summarize the contribution of key facial regions to a specific disease prediction, we propose a face region relevance score that can be applied to any saliency method. We also investigate how prior knowledge, namely human phenotype ontology and DNN model explanations, align. Quantitative experiments are performed on a new database containing over 3,500 images of 11 rare facial syndromes, a healthy control group, and an additional test set of 171 facial images, whose respective facial phenotypes are labeled by clinicians. Current saliency methods are good at capturing dysmorphism in particular regions (parts of the face), but they may not completely capture all the relevant features in a given person or condition. Our study indicates which saliency explanations and face regions are more consistent with the phenotypes of specific genetic syndromes and could be used in large-scale clinical evaluations.\n\n\n\n\n\n\n\n\n\n Ö. Sümer, F. Hellmann, A. Hustinx, T.-C. Hsieh, E. André, and K. Peter, “Few-shot meta-learning for recognizing facial phenotypes of genetic disorders,” vol. Caring is Sharing – Exploiting the Value in Data for Health and Innovation, pp. 932–936, 2023. \nPaper Database (GestaltMatcher DB)\n\nAbstract: Computer vision has useful applications in precision medicine and recognizing facial phenotypes of genetic disorders is one of them. Many genetic disorders are known to affect faces’ visual appearance and geometry. Automated classification and similarity retrieval aid physicians in decision-making to diagnose possible genetic conditions as early as possible. Previous work has addressed the problem as a classification problem; however, the sparse label distribution, having few labeled samples, and huge class imbalances across categories make representation learning and generalization harder. In this study, we used a facial recognition model trained on a large corpus of healthy individuals as a pre-task and transferred it to facial phenotype recognition. Furthermore, we created simple baselines of few-shot meta-learning methods to improve our base feature descriptor. Our quantitative results on GestaltMatcher Database (GMDB) show that our CNN baseline surpasses previous works, including GestaltMatcher, and few-shot meta-learning strategies improve retrieval performance in frequent and rare classes.\n\n\n\n\n\nPublications\nFor further information, please see my Google Scholar profile.\n\n\n[1] Ö. Sümer et al., “Region-based saliency explanations on the recognition of facial genetic syndromes,” in Machine learning for healthcare conference, PMLR, 2023, pp. 1–29.\n\n\n[2] Ö. Sümer, F. Hellmann, A. Hustinx, T.-C. Hsieh, E. André, and K. Peter, “Few-shot meta-learning for recognizing facial phenotypes of genetic disorders,” vol. Caring is Sharing – Exploiting the Value in Data for Health and Innovation, pp. 932–936, 2023, doi: 10.3233/SHTI230312.\n\n\n[3] A. Hustinx et al., “Improving deep facial phenotyping for ultra-rare disorder verification using model ensembles,” in 2023 IEEE/CVF winter conference on applications of computer vision (WACV), 2023, pp. 5007–5017. doi: 10.1109/WACV56688.2023.00499.\n\n\n[4] M. K. Tellamekala, Ö. Sümer, B. W. Schuller, E. André, T. Giesbrecht, and M. Valstar, “Are 3D face shapes expressive enough for recognising continuous emotions and action unit intensities?” IEEE Transactions on Affective Computing, pp. 1–14, 2023, doi: 10.1109/TAFFC.2023.3280530.\n\n\n[5] Ö. Sümer, C. Beyan, F. Ruth, O. Kramer, U. Trautwein, and E. Kasneci, “Estimating presentation competence using multimodal nonverbal behavioral cues,” arXiv preprint arXiv:2105.02636, 2021.\n\n\n[6] Ö. Sümer, E. Bozkir, T. Kübler, S. Grüner, S. Utz, and E. Kasneci, “FakeNewsPerception: An eye movement dataset on the perceived believability of news stories,” Data in Brief, vol. 35, p. 106909, 2021, doi: https://doi.org/10.1016/j.dib.2021.106909.\n\n\n[7] Ö. Sümer, P. Goldberg, S. D’Mello, P. Gerjets, U. Trautwein, and E. Kasneci, “Multimodal engagement analysis from facial videos in the classroom,” IEEE Transactions on Affective Computing, vol. 14, no. 2, pp. 1012–1027, 2023, doi: 10.1109/TAFFC.2021.3127692.\n\n\n[8] Ö. Sümer et al., “Region-based saliency explanations on the recognition of facial genetic syndromes,” in AAAI workshops, 2020. Available: https://arxiv.org/abs/2001.05080\n\n\n[9] Ö. Sümer, P. Gerjets, U. Trautwein, and E. Kasneci, “Attention flow: End-to-end joint attention estimation,” in 2020 IEEE winter conference on applications of computer vision (WACV), 2020, pp. 3316–3325. doi: 10.1109/WACV45572.2020.9093515.\n\n\n[10] P. Goldberg et al., “Attentive or not? Toward a machine learning approach to assessing students’ visible engagement in classroom instruction,” Educational Psychology Review, vol. 33, pp. 27–49, 2021, doi: 10.1007/s10648-019-09514-z.\n\n\n[11] Ö. Sümer et al., “Teachers’ perception in the classroom,” in Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR) workshops, 2018.\n\n\n[12] Ö. Sümer, T. Dencker, and B. Ommer, “Self-supervised learning of pose embeddings from spatiotemporal relations in videos,” in 2017 IEEE international conference on computer vision (ICCV), 2017, pp. 4308–4317. doi: 10.1109/ICCV.2017.461.\n\n\n[13] Ö. Sümer, “Multimodal visual sensing: Automated estimation of engagement,” PhD thesis, Universität Tübingen, 2021. doi: 10.15496/publikation-55003."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ömer Sümer",
    "section": "",
    "text": "recent news  \n\n\nAugust 2023\n\nI presented our paper, “Region-based Saliency Explanations on the Recognition of Facial Genetic Syndromes”, at Machine Learning for Healthcare Conference (MLHC) at Columbia University.\n\n\n\n\n\n\n\n  Region-based Saliency Explanations on the Recognition of Facial Genetic Syndromes   Ömer Sümer, Rebekah L. Waikel, Suzanna E. Ledgister Hanchard, Dat Duong, Peter Krawitz, Cristina Conati, Benjamin D. Solomon, Elisabeth André.  Machine Learning for Healthcare Conference, 2023.\nPaper Poster Database Code"
  }
]